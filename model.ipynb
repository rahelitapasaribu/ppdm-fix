{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063ef8b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as a dll could not be loaded.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresDllLoad'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Install packages jika belum ada\n",
    "try:\n",
    "    import lpips\n",
    "except ImportError:\n",
    "    !pip install lpips\n",
    "\n",
    "try:\n",
    "    from skimage.metrics import structural_similarity as ssim\n",
    "    from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "except ImportError:\n",
    "    !pip install scikit-image\n",
    "\n",
    "import lpips\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9039fe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfigurasi\n",
    "DATASET_PATH = \"processed_dataset\"\n",
    "MODEL_PATH = \"models\"\n",
    "RESULTS_PATH = \"results\"\n",
    "SCALE_FACTOR = 4\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Buat folder untuk model dan hasil\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "os.makedirs(RESULTS_PATH, exist_ok=True)\n",
    "\n",
    "# GPU configuration\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        print(\"‚úÖ GPU configured successfully\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU configuration error: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No GPU found, using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf21558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_info():\n",
    "    \"\"\"Load informasi dataset\"\"\"\n",
    "    with open(os.path.join(DATASET_PATH, \"dataset_info.json\"), \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "dataset_info = load_dataset_info()\n",
    "print(\"Dataset Info:\")\n",
    "for key, value in dataset_info.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eea51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_generator(split=\"train\", batch_size=BATCH_SIZE):\n",
    "    \"\"\"Generator untuk loading data secara batch\"\"\"\n",
    "    high_res_path = os.path.join(DATASET_PATH, split, \"high_res\")\n",
    "    low_res_path = os.path.join(DATASET_PATH, split, \"low_res\")\n",
    "    \n",
    "    # Dapatkan list file\n",
    "    files = sorted(os.listdir(high_res_path))\n",
    "    \n",
    "    def generator():\n",
    "        while True:\n",
    "            # Shuffle files untuk training\n",
    "            if split == \"train\":\n",
    "                np.random.shuffle(files)\n",
    "            \n",
    "            for i in range(0, len(files), batch_size):\n",
    "                batch_files = files[i:i+batch_size]\n",
    "                \n",
    "                lr_batch = []\n",
    "                hr_batch = []\n",
    "                \n",
    "                for filename in batch_files:\n",
    "                    # Load low-res image\n",
    "                    lr_img = cv2.imread(os.path.join(low_res_path, filename))\n",
    "                    lr_img = cv2.cvtColor(lr_img, cv2.COLOR_BGR2RGB)\n",
    "                    lr_img = lr_img.astype(np.float32) / 255.0\n",
    "                    \n",
    "                    # Load high-res image\n",
    "                    hr_img = cv2.imread(os.path.join(high_res_path, filename))\n",
    "                    hr_img = cv2.cvtColor(hr_img, cv2.COLOR_BGR2RGB)\n",
    "                    hr_img = hr_img.astype(np.float32) / 255.0\n",
    "                    \n",
    "                    lr_batch.append(lr_img)\n",
    "                    hr_batch.append(hr_img)\n",
    "                \n",
    "                yield np.array(lr_batch), np.array(hr_batch)\n",
    "    \n",
    "    return generator, len(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18f6c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_espcn_model():\n",
    "    \"\"\"Build model ESPCN\"\"\"\n",
    "    inputs = keras.Input(shape=(64, 64, 3))\n",
    "    \n",
    "    # Feature extraction layers\n",
    "    x = layers.Conv2D(64, 5, padding='same', activation='relu')(inputs)\n",
    "    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "    x = layers.Conv2D(32, 3, padding='same', activation='relu')(x)\n",
    "    \n",
    "    # Sub-pixel convolution layer\n",
    "    # For 4x upscaling, we need 4^2 * 3 = 48 channels\n",
    "    x = layers.Conv2D(SCALE_FACTOR * SCALE_FACTOR * 3, 3, padding='same')(x)\n",
    "    \n",
    "    # Sub-pixel shuffle (pixel shuffle)\n",
    "    outputs = tf.nn.depth_to_space(x, SCALE_FACTOR)\n",
    "    \n",
    "    model = keras.Model(inputs, outputs, name=\"ESPCN\")\n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "model = build_espcn_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb712049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a624e42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data generators\n",
    "train_generator, train_steps = create_data_generator(\"train\", BATCH_SIZE)\n",
    "test_generator, test_steps = create_data_generator(\"test\", BATCH_SIZE)\n",
    "\n",
    "print(f\"Training steps per epoch: {train_steps // BATCH_SIZE}\")\n",
    "print(f\"Testing steps per epoch: {test_steps // BATCH_SIZE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a66c822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    os.path.join(MODEL_PATH, \"espcn_best.h5\"),\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=10,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caaa5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model\n",
    "print(\"üöÄ Starting training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator(),\n",
    "    steps_per_epoch=train_steps // BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=test_generator(),\n",
    "    validation_steps=test_steps // BATCH_SIZE,\n",
    "    callbacks=[checkpoint_callback, early_stopping, lr_scheduler],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"‚úÖ Training completed in {training_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b576ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training dan validation loss\"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mae'], label='Training MAE')\n",
    "    plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "    plt.title('Model MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_PATH, \"training_history.png\"))\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623ea818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_weights(os.path.join(MODEL_PATH, \"espcn_best.h5\"))\n",
    "print(\"‚úÖ Best model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db08cd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementasi baseline methods untuk perbandingan\n",
    "def bicubic_upscale(img, scale_factor):\n",
    "    \"\"\"Bicubic upscaling baseline\"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    new_h, new_w = h * scale_factor, w * scale_factor\n",
    "    return cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "def bilinear_upscale(img, scale_factor):\n",
    "    \"\"\"Bilinear upscaling baseline\"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    new_h, new_w = h * scale_factor, w * scale_factor\n",
    "    return cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_LINEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecc6091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluasi metrics\n",
    "def calculate_psnr(img1, img2):\n",
    "    \"\"\"Calculate PSNR\"\"\"\n",
    "    img1 = (img1 * 255).astype(np.uint8)\n",
    "    img2 = (img2 * 255).astype(np.uint8)\n",
    "    return psnr(img1, img2, data_range=255)\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    \"\"\"Calculate SSIM\"\"\"\n",
    "    img1 = (img1 * 255).astype(np.uint8)\n",
    "    img2 = (img2 * 255).astype(np.uint8)\n",
    "    return ssim(img1, img2, data_range=255, channel_axis=2)\n",
    "\n",
    "def calculate_lpips(img1, img2, lpips_model):\n",
    "    \"\"\"Calculate LPIPS\"\"\"\n",
    "    # Convert to tensor format for LPIPS\n",
    "    img1_tensor = tf.convert_to_tensor(img1 * 2.0 - 1.0)  # Normalize to [-1, 1]\n",
    "    img2_tensor = tf.convert_to_tensor(img2 * 2.0 - 1.0)\n",
    "    \n",
    "    # Add batch dimension\n",
    "    img1_tensor = tf.expand_dims(img1_tensor, 0)\n",
    "    img2_tensor = tf.expand_dims(img2_tensor, 0)\n",
    "    \n",
    "    # Transpose to NCHW format\n",
    "    img1_tensor = tf.transpose(img1_tensor, [0, 3, 1, 2])\n",
    "    img2_tensor = tf.transpose(img2_tensor, [0, 3, 1, 2])\n",
    "    \n",
    "    return lpips_model(img1_tensor, img2_tensor).numpy().item()\n",
    "\n",
    "# Initialize LPIPS model\n",
    "lpips_model = lpips.LPIPS(net='alex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c006ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model():\n",
    "    \"\"\"Evaluasi model pada test set\"\"\"\n",
    "    test_files = sorted(os.listdir(os.path.join(DATASET_PATH, \"test\", \"high_res\")))\n",
    "    \n",
    "    espcn_psnr = []\n",
    "    espcn_ssim = []\n",
    "    espcn_lpips = []\n",
    "    \n",
    "    bicubic_psnr = []\n",
    "    bicubic_ssim = []\n",
    "    bicubic_lpips = []\n",
    "    \n",
    "    bilinear_psnr = []\n",
    "    bilinear_ssim = []\n",
    "    bilinear_lpips = []\n",
    "    \n",
    "    print(\"üîç Evaluating model...\")\n",
    "    \n",
    "    for filename in tqdm(test_files[:50]):  # Evaluate first 50 images\n",
    "        # Load images\n",
    "        hr_path = os.path.join(DATASET_PATH, \"test\", \"high_res\", filename)\n",
    "        lr_path = os.path.join(DATASET_PATH, \"test\", \"low_res\", filename)\n",
    "        \n",
    "        hr_img = cv2.imread(hr_path)\n",
    "        lr_img = cv2.imread(lr_path)\n",
    "        \n",
    "        hr_img = cv2.cvtColor(hr_img, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "        lr_img = cv2.cvtColor(lr_img, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "        \n",
    "        # ESPCN prediction\n",
    "        lr_batch = np.expand_dims(lr_img, axis=0)\n",
    "        espcn_pred = model.predict(lr_batch, verbose=0)[0]\n",
    "        espcn_pred = np.clip(espcn_pred, 0, 1)\n",
    "        \n",
    "        # Baseline methods\n",
    "        bicubic_pred = bicubic_upscale(lr_img, SCALE_FACTOR)\n",
    "        bilinear_pred = bilinear_upscale(lr_img, SCALE_FACTOR)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        # ESPCN\n",
    "        espcn_psnr.append(calculate_psnr(hr_img, espcn_pred))\n",
    "        espcn_ssim.append(calculate_ssim(hr_img, espcn_pred))\n",
    "        espcn_lpips.append(calculate_lpips(hr_img, espcn_pred, lpips_model))\n",
    "        \n",
    "        # Bicubic\n",
    "        bicubic_psnr.append(calculate_psnr(hr_img, bicubic_pred))\n",
    "        bicubic_ssim.append(calculate_ssim(hr_img, bicubic_pred))\n",
    "        bicubic_lpips.append(calculate_lpips(hr_img, bicubic_pred, lpips_model))\n",
    "        \n",
    "        # Bilinear\n",
    "        bilinear_psnr.append(calculate_psnr(hr_img, bilinear_pred))\n",
    "        bilinear_ssim.append(calculate_ssim(hr_img, bilinear_pred))\n",
    "        bilinear_lpips.append(calculate_lpips(hr_img, bilinear_pred, lpips_model))\n",
    "    \n",
    "    # Calculate averages\n",
    "    results = {\n",
    "        \"ESPCN\": {\n",
    "            \"PSNR\": np.mean(espcn_psnr),\n",
    "            \"SSIM\": np.mean(espcn_ssim),\n",
    "            \"LPIPS\": np.mean(espcn_lpips)\n",
    "        },\n",
    "        \"Bicubic\": {\n",
    "            \"PSNR\": np.mean(bicubic_psnr),\n",
    "            \"SSIM\": np.mean(bicubic_ssim),\n",
    "            \"LPIPS\": np.mean(bicubic_lpips)\n",
    "        },\n",
    "        \"Bilinear\": {\n",
    "            \"PSNR\": np.mean(bilinear_psnr),\n",
    "            \"SSIM\": np.mean(bilinear_ssim),\n",
    "            \"LPIPS\": np.mean(bilinear_lpips)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_results = evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7a3669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print evaluation results\n",
    "print(\"=\"*60)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for method, metrics in evaluation_results.items():\n",
    "    print(f\"\\n{method}:\")\n",
    "    print(f\"  PSNR: {metrics['PSNR']:.4f} dB\")\n",
    "    print(f\"  SSIM: {metrics['SSIM']:.4f}\")\n",
    "    print(f\"  LPIPS: {metrics['LPIPS']:.4f}\")\n",
    "\n",
    "# Save results\n",
    "with open(os.path.join(RESULTS_PATH, \"evaluation_results.json\"), \"w\") as f:\n",
    "    json.dump(evaluation_results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6444acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_comparison():\n",
    "    \"\"\"Plot comparison metrics\"\"\"\n",
    "    methods = list(evaluation_results.keys())\n",
    "    psnr_values = [evaluation_results[method][\"PSNR\"] for method in methods]\n",
    "    ssim_values = [evaluation_results[method][\"SSIM\"] for method in methods]\n",
    "    lpips_values = [evaluation_results[method][\"LPIPS\"] for method in methods]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # PSNR\n",
    "    axes[0].bar(methods, psnr_values, color=['red', 'blue', 'green'])\n",
    "    axes[0].set_title('PSNR Comparison')\n",
    "    axes[0].set_ylabel('PSNR (dB)')\n",
    "    axes[0].set_ylim(0, max(psnr_values) * 1.1)\n",
    "    \n",
    "    # SSIM\n",
    "    axes[1].bar(methods, ssim_values, color=['red', 'blue', 'green'])\n",
    "    axes[1].set_title('SSIM Comparison')\n",
    "    axes[1].set_ylabel('SSIM')\n",
    "    axes[1].set_ylim(0, 1)\n",
    "    \n",
    "    # LPIPS (lower is better)\n",
    "    axes[2].bar(methods, lpips_values, color=['red', 'blue', 'green'])\n",
    "    axes[2].set_title('LPIPS Comparison (Lower is Better)')\n",
    "    axes[2].set_ylabel('LPIPS')\n",
    "    axes[2].set_ylim(0, max(lpips_values) * 1.1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_PATH, \"metrics_comparison.png\"))\n",
    "    plt.show()\n",
    "\n",
    "plot_metrics_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090e010c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisasi hasil prediksi\n",
    "def visualize_predictions(num_samples=3):\n",
    "    \"\"\"Visualisasi prediksi model\"\"\"\n",
    "    test_files = sorted(os.listdir(os.path.join(DATASET_PATH, \"test\", \"high_res\")))\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 5, figsize=(20, 4*num_samples))\n",
    "    fig.suptitle(\"Model Predictions Comparison\", fontsize=16)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        filename = test_files[i]\n",
    "        \n",
    "        # Load images\n",
    "        hr_path = os.path.join(DATASET_PATH, \"test\", \"high_res\", filename)\n",
    "        lr_path = os.path.join(DATASET_PATH, \"test\", \"low_res\", filename)\n",
    "        \n",
    "        hr_img = cv2.imread(hr_path)\n",
    "        lr_img = cv2.imread(lr_path)\n",
    "        \n",
    "        hr_img = cv2.cvtColor(hr_img, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "        lr_img = cv2.cvtColor(lr_img, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "        \n",
    "        # Predictions\n",
    "        lr_batch = np.expand_dims(lr_img, axis=0)\n",
    "        espcn_pred = model.predict(lr_batch, verbose=0)[0]\n",
    "        espcn_pred = np.clip(espcn_pred, 0, 1)\n",
    "        \n",
    "        bicubic_pred = bicubic_upscale(lr_img, SCALE_FACTOR)\n",
    "        bilinear_pred = bilinear_upscale(lr_img, SCALE_FACTOR)\n",
    "        \n",
    "        # Plot\n",
    "        axes[i, 0].imshow(lr_img)\n",
    "        axes[i, 0].set_title(\"Low-Res Input\")\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        axes[i, 1].imshow(bilinear_pred)\n",
    "        axes[i, 1].set_title(\"Bilinear\")\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        axes[i, 2].imshow(bicubic_pred)\n",
    "        axes[i, 2].set_title(\"Bicubic\")\n",
    "        axes[i, 2].axis('off')\n",
    "        \n",
    "        axes[i, 3].imshow(espcn_pred)\n",
    "        axes[i, 3].set_title(\"ESPCN\")\n",
    "        axes[i, 3].axis('off')\n",
    "        \n",
    "        axes[i, 4].imshow(hr_img)\n",
    "        axes[i, 4].set_title(\"Ground Truth\")\n",
    "        axes[i, 4].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_PATH, \"predictions_comparison.png\"), dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "visualize_predictions(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45117eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join(MODEL_PATH, \"espcn_final.h5\"))\n",
    "print(\"‚úÖ Model saved for GUI application\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e83705c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training summary\n",
    "def save_training_summary():\n",
    "    \"\"\"Save training summary\"\"\"\n",
    "    summary = {\n",
    "        \"model_name\": \"ESPCN\",\n",
    "        \"scale_factor\": SCALE_FACTOR,\n",
    "        \"training_time\": training_time,\n",
    "        \"epochs_trained\": len(history.history['loss']),\n",
    "        \"best_val_loss\": min(history.history['val_loss']),\n",
    "        \"evaluation_results\": evaluation_results,\n",
    "        \"dataset_info\": dataset_info,\n",
    "        \"hyperparameters\": {\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "            \"optimizer\": \"Adam\",\n",
    "            \"loss_function\": \"MSE\"\n",
    "        },\n",
    "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(RESULTS_PATH, \"training_summary.json\"), \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(\"Training Summary:\")\n",
    "    print(f\"  Model: {summary['model_name']}\")\n",
    "    print(f\"  Scale Factor: {summary['scale_factor']}x\")\n",
    "    print(f\"  Training Time: {summary['training_time']:.2f} seconds\")\n",
    "    print(f\"  Epochs: {summary['epochs_trained']}\")\n",
    "    print(f\"  Best Validation Loss: {summary['best_val_loss']:.6f}\")\n",
    "    print(f\"  Timestamp: {summary['timestamp']}\")\n",
    "\n",
    "save_training_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf79039c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference speed\n",
    "def test_inference_speed():\n",
    "    \"\"\"Test inference speed\"\"\"\n",
    "    test_file = sorted(os.listdir(os.path.join(DATASET_PATH, \"test\", \"low_res\")))[0]\n",
    "    lr_path = os.path.join(DATASET_PATH, \"test\", \"low_res\", test_file)\n",
    "    \n",
    "    lr_img = cv2.imread(lr_path)\n",
    "    lr_img = cv2.cvtColor(lr_img, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "    lr_batch = np.expand_dims(lr_img, axis=0)\n",
    "    \n",
    "    # Warm up\n",
    "    for _ in range(5):\n",
    "        _ = model.predict(lr_batch, verbose=0)\n",
    "    \n",
    "    # Measure inference time\n",
    "    times = []\n",
    "    for _ in range(100):\n",
    "        start_time = time.time()\n",
    "        _ = model.predict(lr_batch, verbose=0)\n",
    "        times.append(time.time() - start_time)\n",
    "    \n",
    "    avg_time = np.mean(times)\n",
    "    fps = 1.0 / avg_time\n",
    "    \n",
    "    print(f\"Inference Speed:\")\n",
    "    print(f\"  Average time: {avg_time:.4f} seconds\")\n",
    "    print(f\"  FPS: {fps:.2f}\")\n",
    "    \n",
    "    return avg_time, fps\n",
    "\n",
    "inference_time, fps = test_inference_speed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4889480",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppdm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
